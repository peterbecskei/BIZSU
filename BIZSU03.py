import csv
import requests
#from bs4 import BeautifulSoup
import os
import html
import time
import sys
from datetime import datetime
import json
import re
from typing import List, Dict, Any

#html_content = ""
# f√°jlnevek
INPUT_FILE = "URL_LIST.csv"
OUTPUT_FILE = "ADAT.csv"


def extract_product_data(html_content: str) -> Dict[str, Any]:
    """
    Kinyeri a term√©kadatokt a HTML-b≈ël
    """
    # Regex minta a products adatok kinyer√©s√©re
    products_pattern = r'products="({.*?})"'
    products_match = re.search(products_pattern, html_content, re.DOTALL)

    if not products_match:
        return {}

    try:
        # JSON dek√≥dol√°s (HTML entity-k helyettes√≠t√©se)
        products_json = products_match.group(1)
        products_json = products_json.replace('&quot;', '"')
        products_data = json.loads(products_json)

        return products_data
    except json.JSONDecodeError as e:
        print(f"JSON dek√≥dol√°si hiba: {e}")
        return {}


def parse_products(products_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Feldolgozza a term√©kadatokat
    """
    products = []

    if 'data' not in products_data:
        return products

    for product_group in products_data['data']:
        for product in product_group:
            # Alap term√©k inform√°ci√≥k
            product_info = {
                'cikkszam': product.get('CikkKod', ''),
                'termek_nev': product.get('CnevText', ''),
                'gyarto': product.get('Gyarto', ''),
                'ar_netto': product.get('KiskerAr', ''),
                'ar_brutto': product.get('br_full_price', ''),
                'keszlet': product.get('Keszlet', ''),
                'kep': product.get('Kep1', ''),
                'tcd_artnr': product.get('TCD_ARTNR', ''),
                'tcd_gyarto': product.get('TCD_GYARTO', ''),
                'seo_url': product.get('seo', ''),
                'keszlet_info': product.get('stock_html', '')
            }
            products.append(product_info)

    return products


def extract_all_variables(html_content: str) -> Dict[str, Any]:
    """
    Minden v√°ltoz√≥t kinyer a HTML-b≈ël
    """
    result = {}

    # Products adatok
    products_data = extract_product_data(html_content)
    if products_data:
        result['products'] = parse_products(products_data)
        result['pagination'] = {
            'current_page': products_data.get('current_page', 1),
            'last_page': products_data.get('last_page', 1),
            'total': products_data.get('total', 0),
            'per_page': products_data.get('per_page', 10),
            'next_page_url': products_data.get('next_page_url', '')
        }

    # Egy√©b v√°ltoz√≥k kinyer√©se
    patterns = {
        'productnumbers': r'productnumbers="(\[.*?\])"',
        'manufacturers': r'manufacturers="({.*?})"',
        'filtersconfig': r'filtersconfig="({.*?})"',
        'searchtype': r'searchtype="(\d+)"',
        'searchquery': r'searchquery="([^"]*)"'
    }

    for key, pattern in patterns.items():
        match = re.search(pattern, html_content, re.DOTALL)
        if match:
            try:
                if key in ['manufacturers', 'filtersconfig']:
                    # JSON objektumok
                    json_str = match.group(1).replace('&quot;', '"')
                    result[key] = json.loads(json_str)
                elif key == 'productnumbers':
                    # JSON t√∂mb
                    json_str = match.group(1).replace('&quot;', '"')
                    result[key] = json.loads(json_str)
                else:
                    # Egyszer≈± string √©rt√©kek
                    result[key] = match.group(1)
            except json.JSONDecodeError:
                result[key] = match.group(1)

    return result


# Haszn√°lat
if __name__ == "__main__":
    # A HTML tartalmat ide kell beilleszteni


    # --- 1. URL_LIST bet√∂lt√©se ---
    url_list = []
    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        reader = csv.reader(f)
        for row in reader:
            if row:  # ne vegye fel az √ºres sorokat
                url_list.append(row[0].strip())

    # --- 2. Site view lek√©r√©se ---
    url_texts = []
    url_extracted = []
    i = 0
    timestamp = time.time()
    for url in url_list:
        try:
            i = i + 1
            print(f"Feldolgoz√°s alatt: {url}")

            resp = requests.get(url, timeout=5)
            # print(type(resp))
            #print(resp.text)
            # Adatok kinyer√©se
            extracted_data = extract_all_variables(resp.text)

            # Eredm√©nyek megjelen√≠t√©se
            print("=== KINYERT ADATOK ===")
            print(f"Term√©kek sz√°ma: {len(extracted_data.get('products', []))}")
            print(f"Oldal inform√°ci√≥s: {extracted_data.get('pagination', {})}")
            print(f"Keres√©si param√©terek: {extracted_data.get('searchquery', '')}")

            # Els≈ë term√©k r√©szletei
            if extracted_data.get('products'):
                first_product = extracted_data['products'][0]
                print("\n=== EL≈êS≈ê TERM√âK ===")
                for key, value in first_product.items():
                    print(f"{key}: {value}")
            print("ok")
            url_extracted.append(extracted_data['products'][0])
            url_texts.append("https://zsu.hu/termeklap/" + extracted_data['products'][0]['seo_url'])


        except Exception as e:
            url_texts.append(f"Hiba: {e}")
    with open(OUTPUT_FILE, "w", encoding="utf-8", newline="") as f:
            writer = csv.writer(f)
            # writer.writerow(["URL_LIST", "URL_TEXT"])  # fejl√©c
            for url, text, extract in zip(url_list, url_texts, url_extracted):
                writer.writerow([url, text, extract])

print(f"‚úÖ üöÄ IlK√©sz! Az eredm√©ny az {os.path.abspath(OUTPUT_FILE)} f√°jlban tal√°lhat√≥.")