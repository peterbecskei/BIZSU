import csv
import requests
#from bs4 import BeautifulSoup
import os
import html
import time
import sys
from datetime import datetime
import json
import re
from typing import List, Dict, Any

#html_content = ""
# f√°jlnevek
INPUT_FILE = "URL_LIST.csv"
OUTPUT_FILE = "ADAT.csv"
TERMEK_FILE = "TERMEK_URL.csv"
BASE_URL = "https://zsu.hu/termeklap/"
termek_urls = []

def extract_product_data(html_content: str) -> Dict[str, Any]:
    """
    Kinyeri a term√©kadatokt a HTML-b≈ël

    """
    print("extract_product_data called")
    print(html_content[:500])  # Csak az els≈ë 500 karaktert jelen√≠ti meg a hossz√∫ tartalmak eset√©n
    # Regex minta a products adatok kinyer√©s√©re
    products_pattern = r'products="({.*?})"'
    products_match = re.search(products_pattern, html_content, re.DOTALL)
    print("products_match:", products_match)
    if not products_match:
        return {}


        # JSON dek√≥dol√°s (HTML entity-k helyettes√≠t√©se)
    products_json = products_match.group(1)
    products_json = products_json.replace('&quot;', '"')
    products_data = json.loads(products_json)
    print("products_data keys:", products_data["data"])
    try:
        for product in products_data["data"]:
            print( BASE_URL + product[0]["seo"], product[0]["CikkKod"], product[0]["CnevText"])
            termek_urls.append(BASE_URL + product[0]["seo"])
            #print(dir(product))
            #print(f"{key}: {products_data[key]}")
        #print(len(products_data), products_data["data"][1] )
        #print(dir(products_data.data))

        return products_data
    except json.JSONDecodeError as e:
        print(f"JSON dek√≥dol√°si hiba: {e}")
        return {}


def parse_products(products_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Feldolgozza a term√©kadatokat
    """
    products = []

    if 'data' not in products_data:
        return products

    for product_group in products_data['data']:
        for product in product_group:
            # Alap term√©k inform√°ci√≥k
            product_info = {
                'cikkszam': product.get('CikkKod', ''),
                'termek_nev': product.get('CnevText', ''),
                'gyarto': product.get('Gyarto', ''),
                'ar_netto': product.get('KiskerAr', ''),
                'ar_brutto': product.get('br_full_price', ''),
                'keszlet': product.get('Keszlet', ''),
                'kep': product.get('Kep1', ''),
                'tcd_artnr': product.get('TCD_ARTNR', ''),
                'tcd_gyarto': product.get('TCD_GYARTO', ''),
                'seo_url': product.get('seo', ''),
                'keszlet_info': product.get('stock_html', '')
            }
            products.append(product_info)

    return products


def extract_all_variables(html_content: str) -> Dict[str, Any]:
    """
    Minden v√°ltoz√≥t kinyer a HTML-b≈ël
    """
    result = {}

    # Products adatok
    products_data = extract_product_data(html_content)
    print(products_data['last_page'])
    for page in range(2, products_data['last_page'] + 1):
        print(f"Oldal: {page}" ,url + f"&page={page}")
        resp = requests.get(url + f"&page={page}", timeout=5)

        more_products_data = extract_product_data(str(resp.text))
        result['products'] = parse_products(more_products_data)
    if products_data:
        result['products'] = parse_products(products_data)
        result['pagination'] = {
            'current_page': products_data.get('current_page', 1),
            'last_page': products_data.get('last_page', 1),
            'total': products_data.get('total', 0),
            'per_page': products_data.get('per_page', 10),
            'next_page_url': products_data.get('next_page_url', '')
        }

    # Egy√©b v√°ltoz√≥k kinyer√©se
    patterns = {
        'productnumbers': r'productnumbers="(\[.*?\])"',
        'manufacturers': r'manufacturers="({.*?})"',
        'filtersconfig': r'filtersconfig="({.*?})"',
        'searchtype': r'searchtype="(\d+)"',
        'searchquery': r'searchquery="([^"]*)"'
    }

    for key, pattern in patterns.items():
        match = re.search(pattern, html_content, re.DOTALL)
        if match:
            try:
                if key in ['manufacturers', 'filtersconfig']:
                    # JSON objektumok
                    json_str = match.group(1).replace('&quot;', '"')
                    result[key] = json.loads(json_str)
                elif key == 'productnumbers':
                    # JSON t√∂mb
                    json_str = match.group(1).replace('&quot;', '"')
                    result[key] = json.loads(json_str)
                else:
                    # Egyszer≈± string √©rt√©kek
                    result[key] = match.group(1)
            except json.JSONDecodeError:
                result[key] = match.group(1)

    return result


# Haszn√°lat
if __name__ == "__main__":
    # A HTML tartalmat ide kell beilleszteni


    # --- 1. URL_LIST bet√∂lt√©se ---
    url_list = []
    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        reader = csv.reader(f)
        for row in reader:
            if row:  # ne vegye fel az √ºres sorokat
                url_list.append(row[0].strip())

    # --- 2. Site view lek√©r√©se ---
    url_texts = []
    url_extracted = []
    i = 0
    timestamp = time.time()
    for url in url_list:
        try:
            i = i + 1
            print(f"Feldolgoz√°s alatt: {url}")

            resp = requests.get(url, timeout=5)

            # Adatok kinyer√©se
            extracted_data = extract_all_variables(resp.text)

            # Eredm√©nyek megjelen√≠t√©se
            #print("=== KINYERT ADATOK ===")
            #print(f"Term√©kek sz√°ma: {len(extracted_data.get('products', []))}")
            #print(f"Oldal inform√°ci√≥s: {extracted_data.get('pagination', {})}")
            #print(f"Keres√©si param√©terek: {extracted_data.get('searchquery', '')}")

            # Els≈ë term√©k r√©szletei
            #if extracted_data.get('products'):
             #   first_product = extracted_data['products'][0]
                #print("\n=== EL≈êS≈ê TERM√âK ===")
                #for key, value in first_product.items():
                    #print(f"{key}: {value}")
            #print("ok")

        except Exception as e:
           print(f"Hiba: {e}")


    with open(TERMEK_FILE, "w", encoding="utf-8", newline="") as f:
            writer = csv.writer(f)
            for termek_url in termek_urls:
                writer.writerow([termek_url])

print(f"‚úÖ üöÄ IlK√©sz! Az eredm√©ny az {os.path.abspath(OUTPUT_FILE)} f√°jlban tal√°lhat√≥.")