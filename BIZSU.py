import csv
import requests
#from bs4 import BeautifulSoup
import os
import html
import json
import re
import time


#url = "https://zsu.hu/header-search/api/start-product-name-search"
#payload = {"searchString": "302540"}

#url = "https://zsu.hu/header-search/api/getHeaderSearches"
#payload = {"searchString": "302540"}



#response = requests.post(url, json=payload)
#response = requests.post(url)


#print("St√°tusz k√≥d:", response.status_code)
#print("V√°lasz:", response.text)
#system.exit()

# f√°jlnevek
INPUT_FILE = "URL_LIST.csv"
OUTPUT_FILE = "ADAT.csv"

# --- 1. URL_LIST bet√∂lt√©se ---
url_list = []
with open(INPUT_FILE, "r", encoding="utf-8") as f:
    reader = csv.reader(f)
    for row in reader:
        if row:  # ne vegye fel az √ºres sorokat
            url_list.append(row[0].strip())

# --- 2. Site view lek√©r√©se ---
url_texts = []
url_extracted = []
i=0
#timestamp = time.time()
for url in url_list:
    try:
        i=i+1
        print(f"Feldolgoz√°s alatt: {url}")


        #def myfunc(a):
        #    return len(a)
        #x = map(myfunc, ('apple', 'banana', 'cherry'))
        #print(x)
        #print(type(x))
        # convert the map into a list, for readability:
        #print(list(x))

        resp = requests.get(url, timeout=5)
        #print(type(resp))
        #print(type(resp.text))
        #print(type(resp))
        #print(dir(resp))
        print(resp.elapsed)
        #print(resp.headers)

        #print(dir(resp.cookies))

        #print(map(resp))
        #print("resp.cookies.get_dict")
        #print(resp.cookies.get_dict)
        #print(resp.cookies.items)
        #print(resp.cookies.values)

        #print(resp.is_redirect)

        #print(resp._content)
        #print(resp.json)
        #print(resp.links)

        #print(resp.next)

        #print(resp.__sizeof__())
        #print(resp.__static_attributes__)
        #print(resp.__str__)
        #print(resp.headers['Content-Length'])

        #product_search = str(resp.text[1000:2000])
        #print(product_search)
        #print(type(product_search))
        #print(resp.text[154416:3000])

        start_index0 = resp.text.find("productnumbers=")
        start_index1 = resp.text.find("filtersconfig=")
        #start_index = resp.text.find("headerContent")
        start_product_list = resp.text.find("product-list-gate")+18
        end_product_list = resp.text.find("> </product-list-gate")
        #print(start_index)
        procuct_list_content = resp.text[start_product_list:end_product_list]

        extracted_content = resp.text[start_product_list:end_product_list]

        #st = search_content.find('e',0) #  product_search_results_container")
       # print(st)
        print(extracted_content)
        #content = resp.content.replace('e9', '√©')
        #resp.encoding = resp.apparent_encoding  # helyes k√≥dol√°s felismer√©se
        #print(resp.content.decode('utf-8'))
        #content = resp.content.decode('utf-8', errors='ignore')
        #content = decode(resp.content)
        #print(content)
        # HTML elemz√©s

        #soup = BeautifulSoup(resp.text, "html.parser")


        # oldal sz√∂veg (title + body)
        #title = soup.title.string if soup.title else ""
        #body_text = soup.get_text(separator=" ", strip=True)
        #full_text = (title + " " + body_text).strip()

        # csak az els≈ë 200 karakter
        # url_texts.append(full_text[:12000])

        #search_results = soup.find(id="product_search_results_container")
        #search_content = str(search_results)
        #print(search_content)
        # speci√°lis karakterek jav√≠t√°sa
        #st=1000
        #search_content = resp.text[st:st+1000]
        content = (extracted_content.replace('\\u00c1', '√Å').
                   replace('\\u0151', '√∂').
                   replace('\\u00fc', '√º').
                   replace('\\u00e9', '√©').
                   replace('\\u00ed', '√≠').
                   replace('\\u0171', '≈±').
                   replace('\\u00f3', '√≥').
                   replace('\\u00e1', '√°').
                   replace('\\u00e7', '√ß').
                   replace('\\u00f6', '√∂').
                   replace('\\u00fc', '√º').
                   replace('\\u00e4', '√§'))
        content = (content.replace('&quot;', "'").
        replace("\\u0150", "‚úÖ").
        replace("\\u00d6", "√ñ").
        replace("lt;", "<").
        replace("&gt;", ">"))  # .replace("&gt;", ">").replace("&amp;", "&")
        print(content)
        # A fix kulcsok list√°ja
        keys = ['productNumber', "CikkKod", 'stock_html', 'tcd_artnr_seo', 'tcd_gyarto_seo','seo', 'Keszlet', 'Gyarto', 'KiskerAr', 'TCD_ID', 'ArDatum', 'RefeKeszlet', 'AlapEgys', 'Suly', 'Kep1','CnevText','TCD_ARTNR','TCD_DLNR','TCD_GYARTO','TCD_ARTNR_SAJAT','TCD_DLNR_SAJAT','gyarto','tcd_gyarto','br_price','customStockOrder','netFullPrice','br_full_price','discountPercent'
                ,'ADV_TCD_ARTNR','CUSTOM_TCD_ARTNR','alternative_products','askForInformation','cikkkod','productImages','firstImageExt'] #,'upmValue','tcd_artnr_seo','tcd_gyarto_seo','seo','stock_html']
#'CnevText','TCD_ARTNR','TCD_DLNR','TCD_GYARTO','TCD_ARTNR_SAJAT','TCD_DLNR_SAJAT','gyarto','tcd_gyarto','br_price','customStockOrder','netFullPrice','br_full_price','discountPercent','tcd_artnr_seo':'30337_01','tcd_gyarto_seo':'lemf_rder','seo':'lem_3033701_br','stock_html':'&<div class=\'left\'> &<div class=\'display-flex margin-x-auto mb-5\'> &<img src=\'\/themes\/frontend\/images\/stock.png\' width=\'22\' height=\'22\'> &<i>K√©szlet&<\/i> &<div class=\'info-box-btn\'> &<img src=\'\/themes\/frontend\/images\/svg\/info.svg\' width=\'18\' height=\'18\'>         &<span>Munkanapokon megrendel√©st√∂l sz√°m√≠tott 3-5 √≥r√°n bel√ºl a kiv√°lasztott √ºzletben √°tvehet√∂.&<\/span>  &<\/div> &<\/div> &<b class=\'display-flex flex-column flex-column-mobile\'> &<span class=\'display-flex margin-x-auto text-center\'>  &<span class=\'display-flex\'> &<img src=\'\/themes\/frontend\/images\/stock-narancs.svg\' class=\'ok beszallito\' width=\'20px\'> &<span class=\'stock-info\'>3-5 √≥r√°n bel√ºl&<\/span>  &<\/span>  &<\/span> &<\/b> &<\/div> &<div class=\'right\'> &<div class=\'display-flex margin-x-auto mb-5\'> &<img src=\'\/themes\/frontend\/images\/svg\/shipping.svg\' width=\'24\' height=\'24\'> &<i>Sz√°ll√≠t√°s&<\/i> &<div class=\'info-box-btn\'> &<img src=\'\/themes\/frontend\/images\/svg\/info.svg\' width=\'18\' height=\'18\'>  &<span>A term√©k be√©rkez√©s√©t√∂l sz√°m√≠tott 1-2 munkanapon bel√ºl √°tadjuk a rendel√©st a fut√°rszolg√°latnak.&<\/span>  &<\/div> &<\/div> &<b class=\'display-flex flex-column flex-column-mobile\'> &<span class=\'display-flex margin-x-auto text-center\'> &<img src=\'\/themes\/frontend\/images\/stock-narancs.svg\' class=\'ok beszallito\' width=\'20px\'>  &<span class=\'stock-info\'>&<p class='text-left'>1-2 munkanap&<\/p>&<\/span>  &<\/span> &<\/b> &<\/div>'}]],'from':1,'last_page':1,'next_page_url':null,'path':'https:\/\/zsu.hu\/cikkszamkereso','per_page':10,'prev_page_url':null,'to':1,'total':1}" searchquery="LEM_3033701" searchtype="10" type="1"> </product-list-gate> </div>
#{'productNumber': None, 'CikkKod': 'LEM 3033701 *BR', 'Keszlet': 'B', 'Gyarto': 'LEMF√ñRDER', 'KiskerAr': '17770.6386', 'TCD_ID': '35', 'ArDatum': '2025-09-18 21:52:46', 'RefeKeszlet': 'I', 'AlapEgys': '1', 'Suly': '0',
        extracted = {}

        for key in keys:
            # Pattern: 'kulcs':'√©rt√©k' ‚Äì az √©rt√©k single quote-ok k√∂z√∂tt, √©s nem tartalmaz bels≈ë ' -et (ha igen, finom√≠tsd a pattern-t)
            pattern = rf"'{key}':'([^']*)'"
            match = re.search(pattern,content)
            if match:
                extracted[key] = match.group(1)
            else:
                extracted[key] = None  # Ha nincs tal√°lat

        # Ki√≠r√°s (vagy √≠rd f√°jlba/Excelbe ha kell)
        #dt=  time.time() - timestamp


        #print(i,dt, url, extracted)
        print( extracted)
        url_extracted.append(extracted)
        url_texts.append(content)
        #print(search_results)
        #ss1 =ss.replace('\u0151', '√∂')
        #ss2 = ss1.replace('\u00fc', '√º').strip()
        #print(ss)
        # HTML entity dek√≥dol√°s
        #decoded = html.unescape(search_results)

        # UTF-8 karakterek jav√≠t√°sa
        #fixed_text = decoded.encode('latin-1').decode('utf-8', errors='ignore')

        #print(fixed_text)
        # JSON-k√©nt √©rtelmezve
        #try:
        #    json_str = '{"' + search_results.replace('":"', '":"').replace('","', '","') + '}'
        #    data = json.loads(json_str)
        #    print(f"JSON adat: {data}")
        #except:
        #    print("Nem valid JSON form√°tum")
        #print(search_results.prettify())
        #url_texts.append(soup.find(id="product_search_results_container"))

    except Exception as e:
        url_texts.append(f"Hiba: {e}")

# --- 3. ADAT.csv ki√≠r√°s ---
with open(OUTPUT_FILE, "w", encoding="utf-8", newline="") as f:
    writer = csv.writer(f)
    #writer.writerow(["URL_LIST", "URL_TEXT"])  # fejl√©c
    for url, text, extract in zip(url_list, url_texts,url_extracted):
        writer.writerow([url, text,extract])




print(f"‚úÖ üöÄ IlK√©sz! Az eredm√©ny az {os.path.abspath(OUTPUT_FILE)} f√°jlban tal√°lhat√≥.")
